{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oukieAKXF1If"
   },
   "source": [
    "# HPML Final Project\n",
    "# Authors: Rafka Daou, Maria Garmonina, Sarah Korb\n",
    "# Project 4: Exploring Chunked States in Mamba Style Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndKDOQT5TIm9"
   },
   "source": [
    "The following code blocks handle importing the necessary modules required for the program to run.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CqYnHmrYTfvL",
    "outputId": "1d4650c4-b32e-4523-8bdf-2dec928a56ed"
   },
   "outputs": [],
   "source": [
    "# Install the relevant packages for the project.\n",
    "! pip install -e .\n",
    "\n",
    "! pip install pynvml rouge_score\n",
    "\n",
    "! pip install ibm-fms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wbAX8YgI2fT"
   },
   "source": [
    "Clone Repository\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jg1Fki1QI2H9",
    "outputId": "1313698c-2711-4b1c-a88b-6b8df2a654b4"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/rafkamicheldaou/foundation-model-stack.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jynmkQHcJF3n",
    "outputId": "5a2cb37f-9e47-47d8-a939-75be7cdca71f"
   },
   "outputs": [],
   "source": [
    "# 2. Set working directory and fix sys.path\n",
    "import sys, os\n",
    "\n",
    "os.chdir(\"/content/foundation-model-stack\")\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "# 3. Confirm you're using the correct `fms`\n",
    "import fms\n",
    "print(\"Using fms from:\", fms.__file__)  # should point to /content/foundation-model-stack/fms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZWzwQBAM7N1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import psutil\n",
    "import pynvml\n",
    "import pandas as pd\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "from fms.utils.tokenizers import get_tokenizer\n",
    "from fms.utils.generation import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hECsnm-jG-1j"
   },
   "source": [
    "The following code is responsible for loading the IBM Bamba module and configuring key parameters, such as the number of layers the model will use. For the purpose of evaluating throughput and latency, we prioritized a configuration with 4 layers while varying prompt lengths.\n",
    "To address the issue of exploding chunked state strategies, we implemented four distinct module variants: default optimized, optimized, optimized with independent chunking, and optimized diagonal-only.\n",
    "To specify which SSM (Structured State Space Model) module is used during model execution, we manually set the desired module in the statements below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CO7LcjWJT9f"
   },
   "source": [
    "**The model version you use should depend on whether you are benchmarking performance or accuracy. Please ensure you load the appropriate model based on the specific task.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300,
     "referenced_widgets": [
      "54974ba3fe4142f49a1723951e38dcf4",
      "3571028934e14abeb5b7c13ef41bb44e",
      "1940ae9f73924424ab839777c0f506b0",
      "af567ab13f654de38e4603c898cc991a",
      "4e219fc29bf34a2f9b76e91189b22780",
      "69266a7de676410897dbac8ad58323b3",
      "e2fd19f4a1b34cafb4937acd903a7b89",
      "5b83c277c4c44e2fb10e12df726c1af5",
      "1607b2cbc0dd442090b0ef2442c07afa",
      "550aac01467d420c83cdb8b9a8d38f00",
      "cf48cad21f1144f18d3aa1dbf400a354",
      "2a1d6ae7637142fcae41ceb66b436107",
      "e861999245284ce2a40421c2b42251b8",
      "8c24f28576a54a32a090aa1130754074",
      "4ed1d27f5c6f4b4e9eec1b74f8a80538",
      "545b5faf504544d8802c86e39b930165",
      "0b309360c0634891aec76722306b3368",
      "9dc25d5ebccf4753b883831686ef6371",
      "c708855be66449beac9c885e633e80ee",
      "0b275d50d220435ab0afc360308e26eb",
      "a16a5e815824414cacf36ba7c712edfb",
      "47e715de59694b62ba955ec9aa6da5d7"
     ]
    },
    "id": "VihNjeuo5wgF",
    "outputId": "b8bbc207-a421-4cea-95d4-8816858796f9"
   },
   "outputs": [],
   "source": [
    "# # CHANGED MODEL - NOW USING CHUNKED SSM, to use with our repo\n",
    "\n",
    "from fms.modules.ssm import SSM as DefaultSSM\n",
    "import fms.models.bamba as _bamba_mod\n",
    "\n",
    "\n",
    "_bamba_mod.SSM = DefaultSSM # assigning this before fetching the pretrained model to not run into errors\n",
    "\n",
    "# Now load Bamba\n",
    "from fms.models import get_model\n",
    "\n",
    "import torch\n",
    "\n",
    "# Load trimmed model properly\n",
    "model = get_model(\n",
    "    \"hf_configured\",\n",
    "    \"ibm-ai-platform/Bamba-9B\",\n",
    "    device_type=\"cuda\",\n",
    "    data_type=torch.bfloat16,\n",
    "    nlayers=4,\n",
    ")\n",
    "model.config.attn_layer_indices = []\n",
    "\n",
    "\n",
    "print(\"Number of layers:\", len(model.base_model.layers))\n",
    "print(\"Config nlayers:\", model.config.nlayers)\n",
    "print(\"Attention layers indices:\", model.config.attn_layer_indices)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276,
     "referenced_widgets": [
      "11f4c9525a6f439e88aeaaefd1a00dca",
      "b5838dcd660e46b79a27692dc5947816",
      "729eb9c95c024c63a7f8eed68555bc72",
      "0beb90fb1abc402cac83d108aa8f307c",
      "9206c43885ce4bbebee7084f687382db",
      "40da613d36514398b5bb5a1de0e35597",
      "896d673a35aa4a668814c6ec0c909779",
      "4182a565258a4ce7953ffe1a0353d75a",
      "8cfb0868b9bb4934915e7132843fca9d",
      "380b1eac6bf148aba2eb9e49c228a418",
      "6a45ef36fcda44008ce0a6e9d0285f50",
      "ad7d9234cf6843118a246ef51743498f",
      "818843378db94643871c2c5f2220afae",
      "3bae980d8ee14b2d9be8787a624bec35",
      "c8696b26889342b8bda0532af7542e2f",
      "46b261cc6cf340c893176b60ec83d6d8",
      "7c7a31900b2c444190fd054fe512285b",
      "05e86fb8d1eb49cb903c1a0c2b240c50",
      "b7302dbf761241f38e9e0b53d9ae78e7",
      "559addab99454251a1a7dc7da764ef9f",
      "dc9d75d22eb246a7bd657e31da061018",
      "2face10d9a5b4a78962348edb192cb3d",
      "c434542d46e542d1bbe943c3e7db6ff2",
      "63fafd5f45514068b453b37b51fc0687",
      "c2b30a4102414015a49008324ed6151a",
      "0c6c3a316bb543afa0184ba194e33119",
      "d20e13da125a421a8d15bad6ddd1a61b",
      "35a2b3700c854f12a1f34eeb7796a871",
      "f47feeff022c454da0bd385a4546e5f7",
      "ca9acddb3fce456887d67ca3102429a4",
      "729ecb0f1ac34920bf4a26edf1021c47",
      "fab404e0b6f34687b81bb383b4ad0f2c",
      "b95e8c57613a42579c8c4d7c64ee8db1",
      "4ad81d0966a849abb89d7de82d074b4b",
      "29d80df18c604968a7d64fd4cc059164",
      "e14fca524c294f518d23ce962e71edff",
      "7d8f07e51ee941258b9c6fe0989d0329",
      "8e2460effc6c4363addcef12c88365f0",
      "fca382050b7448a3862e2e649a21f9cd",
      "2a64ead1f57a4d97bc2ca9d3483c8c64",
      "f76d835538a54197a2661e27c4906eba",
      "2aa050ef1074472ab2f521bcc82146e4",
      "546f13ce8fa94f3d8f95f4fb26af00da",
      "0be33227b345492889854d0913ab6107",
      "ccdaa5ec13d34444a15d5734ef179e14",
      "947ea11d881d4b729a0e059f1f82465f",
      "650b788f95e041e88681e206304b3520",
      "71edb567f465410ba251e2dae92f7252",
      "dce7fe9c65c947f3a716b1117b7579fa",
      "2f18fe6246184353a6d3c05fc37df526",
      "e952eeaadd6a4c18a4fa725d9f2e71e5",
      "3076d47caddb4017931294e0ec2ec73a",
      "75f37a73928e4e35994789ba7f41ebf6",
      "f6383d7e74984ef68826a02e1d1e6c83",
      "d2c1e2e4757a4f3f8b69687f4d5b60b6",
      "d170e7c7e69f40c5ac2c1265c998ecd3",
      "c68b2266f2074d4e9be8c87ca785f3c3",
      "08b7295adb8b4d3d975fa5be79b85e34",
      "d73e1b05952946dc980eecf1ad9df769",
      "8e463e5998dc41bc83bccae0d191b78d",
      "0d967b6bf9be4d0fbfd96b9b7066ca68",
      "195fc023f8cc4d9aad354b569797e9c3",
      "b0a115b254134c6a82cb7a5c3fc28cc3",
      "d2d0d65e98f842668e21d0a3f49dac88",
      "63cac7ac2ad0447f880a9e7caa3e7ea9",
      "d526ecd8fe0244ac951be5fca56b7bbf"
     ]
    },
    "id": "eeXRlqwXJzrl",
    "outputId": "131fbe93-8348-4459-8175-504baf850334"
   },
   "outputs": [],
   "source": [
    "# CODE TO LOAD FOR ACCURACY BENCHMARKING\n",
    "from fms.modules.default_triton_ssm import SSM as DefaultSSM\n",
    "import fms.models.bamba as _bamba_mod\n",
    "\n",
    "\n",
    "_bamba_mod.ssm = DefaultSSM # assigning this before fetching the pretrained model to not run into errors\n",
    "\n",
    "# Now load Bamba\n",
    "from fms.models import get_model\n",
    "\n",
    "import torch\n",
    "model = get_model(\n",
    "    \"hf_pretrained\",\n",
    "    \"ibm-ai-platform/Bamba-9B-v2\",\n",
    "    device_type=\"cuda\",\n",
    ")\n",
    "model.config.attn_layer_indices = []\n",
    "\n",
    "print(\"Number of layers:\", len(model.base_model.layers), flush=True)\n",
    "print(\"Config nlayers:\", model.config.nlayers, flush=True)\n",
    "print(\"Attention layers indices:\", model.config.attn_layer_indices, flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69W2XZAOpmiN",
    "outputId": "171b6ae0-d1b6-47f2-ff10-86ca9b7d144c"
   },
   "outputs": [],
   "source": [
    "# confirm model parameters\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SXBm-1y_uM2m",
    "outputId": "e57682a7-e657-455d-9cb1-763beb04e6be"
   },
   "outputs": [],
   "source": [
    "# confirm which SSM module is running\n",
    "layer = next(\n",
    "    block for block in model.base_model.layers\n",
    "    if hasattr(block, \"ssm\")\n",
    ")\n",
    "print(isinstance(layer.ssm, DefaultSSM), # PASS IN THE LOADED SSM MODULE HERE TO VERIFY IT IS BEING LOADED PROPERLY\n",
    "      layer.ssm.__class__.__module__ + \".\" + layer.ssm.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmkXI65WGdPA"
   },
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUtrjO8vIL7t"
   },
   "source": [
    "This code benchmarks the performance of different chunked state strategies for the Bamba model. It processes a list of prompts by tokenizing the input, running the model to generate output, and collecting detailed performance metrics. These include total latency, first-token and inter-token generation times, throughput (tokens per second), peak memory usage, memory bandwidth, CPU/GPU utilization, and total FLOPs. The profiler also logs the most time-consuming CUDA operations. All results are recorded for analysis, allowing comparison across different SSM module variants and prompt configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5lE7gjvSOYq"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# updated file containing 16 prompts of lengths [256, 512, 1024, 2048]\n",
    "with open(\"./benchmarking_data/longer_qa_for_benchmarking_performance.json\", \"r\",encoding=\"utf-8\") as f:\n",
    "  qa_pairs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSf4ZT78WFPY"
   },
   "outputs": [],
   "source": [
    "\n",
    "def ids_for_prompt(prompt, tokenizer, device):\n",
    "    toks = tokenizer.tokenize(prompt)\n",
    "    ids  = tokenizer.convert_tokens_to_ids(toks)\n",
    "    return torch.tensor(ids, dtype=torch.long, device=device)\n",
    "\n",
    "def decode_ids(ids):\n",
    "    toks  = tokenizer.convert_ids_to_tokens(ids)\n",
    "    return tokenizer.convert_tokens_to_string(toks)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "tokenizer = get_tokenizer(\"ibm-ai-platform/Bamba-9B\")\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "model.compile()\n",
    "\n",
    "records = []\n",
    "MAX_NEW_TOKENS = 100\n",
    "log_path = \"./benchamrking_data/optimized_default.txt\"\n",
    "with open(log_path, \"a\", encoding=\"utf-8\") as log_file:\n",
    "    for idx, item in enumerate(qa_pairs[:70], start=1):\n",
    "        inputs = ids_for_prompt(item[\"prompt\"], tokenizer, device)\n",
    "\n",
    "        # system stats before\n",
    "        cpu0 = psutil.cpu_percent(None)\n",
    "        io0  = psutil.cpu_times_percent(None).iowait\n",
    "        gpu0 = pynvml.nvmlDeviceGetUtilizationRates(gpu_handle).gpu\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        # profile the generate step\n",
    "        with profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            record_shapes=True,\n",
    "            profile_memory=True,\n",
    "            with_flops=True,\n",
    "        ) as prof:\n",
    "            torch.cuda.synchronize()\n",
    "            t_start = time.time()\n",
    "\n",
    "            out_ids, times = generate(\n",
    "                model,\n",
    "                inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                use_cache=False,\n",
    "                timing=\"per-token\",\n",
    "            )\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            t_end = time.time()\n",
    "\n",
    "        # system stats after\n",
    "        cpu1 = psutil.cpu_percent(None)\n",
    "        io1  = psutil.cpu_times_percent(None).iowait\n",
    "        gpu1 = pynvml.nvmlDeviceGetUtilizationRates(gpu_handle).gpu\n",
    "        peak_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "        # derive metrics\n",
    "        t_first     = times[0]\n",
    "        t_mean      = sum(times[1:]) / len(times[1:])\n",
    "        total_time  = t_end - t_start\n",
    "        throughput  = MAX_NEW_TOKENS / total_time\n",
    "        mem_bw      = peak_mem / total_time\n",
    "        total_flops = sum(evt.flops for evt in prof.key_averages() if hasattr(evt, \"flops\"))\n",
    "        top_ops     = prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=5)\n",
    "\n",
    "        rec = {\n",
    "            \"id\": idx,\n",
    "            \"total_latency_s\": total_time,\n",
    "            \"first_token_s\": t_first,\n",
    "            \"mean_inter_token_s\": t_mean,\n",
    "            \"throughput_tok_s\": throughput,\n",
    "            \"peak_mem_MB\": peak_mem,\n",
    "            \"mem_bw_MBps\": mem_bw,\n",
    "            \"cpu_start_%\": cpu0, \"cpu_end_%\": cpu1,\n",
    "            \"gpu_start_%\": gpu0, \"gpu_end_%\": gpu1,\n",
    "            \"io_wait_diff_%\": io1 - io0,\n",
    "            \"total_flops\": total_flops,\n",
    "            \"profiler_top_ops\": top_ops,\n",
    "            \"output\": decode_ids(out_ids),\n",
    "            \"num_chunks\": item[\"num_chunks\"],\n",
    "            \"token_len\": item[\"token_len\"],\n",
    "            \"prompt\": item[\"prompt\"]\n",
    "        }\n",
    "\n",
    "        log_file.write(f\"{rec}\\n\")\n",
    "        records.append(rec)\n",
    "\n",
    "        print(\n",
    "            f\"{idx}/160 | tot={total_time:.3f}s \"\n",
    "            f\"| first={t_first:.3f}s | inter={t_mean:.4f}s | thr={throughput:.1f} tok/s\",flush=True\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv(\"./benchamrking_data/default_optimized.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqX825G2ID5j"
   },
   "source": [
    "# Accuarcy Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLw83_rBId2C"
   },
   "source": [
    "This code performs benchmarking for evaluating the accuracy of different chunked state strategies in the Bamba model. It processes a list of QA pairs by tokenizing the prompts, generating only the predicted answer portion (based on reference answer length), and decoding the generated output. The results—including prompt, reference answer, and model prediction—are logged to a TSV file for later analysis. This setup isolates the model’s generative accuracy, allowing precise comparisons across chunking strategies while controlling for output length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-L2vEmgUWVsx"
   },
   "outputs": [],
   "source": [
    "with open(\"./benchmarking_data/qa_for_accuracy_256.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_pairs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tV64hefsY-U3"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "tokenizer = get_tokenizer(\"ibm-ai-platform/Bamba-9B\")\n",
    "model.compile()\n",
    "\n",
    "def ids_for_prompt(prompt: str, tokenizer, device):\n",
    "    toks = tokenizer.tokenize(prompt)\n",
    "    ids = tokenizer.convert_tokens_to_ids(toks)\n",
    "    if tokenizer.bos_token_id != tokenizer.eos_token_id:\n",
    "        ids = [tokenizer.bos_token_id] + ids\n",
    "    return torch.tensor(ids, dtype=torch.long, device=device)\n",
    "\n",
    "def decode_ids(ids: torch.Tensor):\n",
    "    toks = tokenizer.convert_ids_to_tokens(ids.tolist())\n",
    "    return tokenizer.convert_tokens_to_string(toks)\n",
    "\n",
    "log_path = \"./benchmarking_data/accuracy_default_triton_256.tsv\"\n",
    "write_header = not os.path.exists(log_path)\n",
    "\n",
    "with open(log_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    if write_header:\n",
    "        writer.writerow([\"id\", \"prompt\", \"reference\", \"prediction\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, item in enumerate(qa_pairs[63:73], start=1):\n",
    "            prompt = item[\"prompt\"]\n",
    "            inputs = ids_for_prompt(prompt, tokenizer, device)\n",
    "            prompt_len = inputs.size(0)\n",
    "            prompt_len = inputs.size(0)\n",
    "            answer_len = len(tokenizer.tokenize(item[\"answer\"]))\n",
    "            total_len = prompt_len + answer_len\n",
    "\n",
    "            print(f\"Prompt {idx} | Prompt tokens: {prompt_len} | Answer tokens: {answer_len} | Total: {total_len}\",flush=True)\n",
    "            out_ids = generate(\n",
    "                model,\n",
    "                inputs,\n",
    "                max_new_tokens=len(tokenizer.tokenize(item[\"answer\"])),\n",
    "                use_cache=True,\n",
    "                timing=\"\",\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "            new_ids = out_ids[prompt_len:]\n",
    "            output_text = decode_ids(new_ids)\n",
    "            reference = item[\"answer\"]\n",
    "\n",
    "            writer.writerow([idx, prompt, reference, output_text])\n",
    "            print(f\"{idx:3d}: {output_text}…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6fH_RlzEAmN"
   },
   "source": [
    "## Accuracy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McUi0bCbJNKI"
   },
   "source": [
    "This section prepares the prediction results for accuracy evaluation using GPTScore, a reference-based automatic metric that leverages GPT models to assess the quality of generated text. GPTScore evaluates how well the system’s predicted answers (summaries) align with human-written references. The code loads the predictions TSV, formats each record into the expected JSON schema, and saves it to the directory. Each entry includes the prompt, reference, prediction, and metadata such as system name and evaluation aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPQYJrsOhjA8",
    "outputId": "cb4bde74-624f-44f4-cc3d-a152a5f28f53"
   },
   "outputs": [],
   "source": [
    "# using gpt scorer [after benchmarking]\n",
    "\n",
    "!git clone https://github.com/jinlanfu/GPTScore.git\n",
    "%cd GPTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeaWwiym4VnR"
   },
   "outputs": [],
   "source": [
    "# then put the predictions tsv into:\n",
    "\n",
    "import os, json, pandas as pd\n",
    "\n",
    "# file path needs to be updated\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/predictions_chunkingtype.tsv\", sep=\"\\t\")\n",
    "\n",
    "# build the JSON structure\n",
    "records = []\n",
    "for _, row in df.iterrows():\n",
    "    records.append({\n",
    "        \"src\": row[\"prompt\"],\n",
    "        \"ref_summ\": row[\"reference\"],\n",
    "        \"sys_summ\": row[\"prediction\"],\n",
    "        \"sys_name\": \"chunkingtype\",\n",
    "        \"aspect\": \"quality\",\n",
    "        \"polarity\": \"positive\"\n",
    "    })\n",
    "\n",
    "demo = {\n",
    "    \"demo\": {\n",
    "        \"quality\": records\n",
    "    },\n",
    "    \"asp_definition\": {\n",
    "        \"quality\": \"Convert the following text into another expression that is fluent and grammatically correct:\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# write it where the CLI expects it\n",
    "out_dir = \"GPTScore/datas/chunkingtype\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "with open(os.path.join(out_dir, \"demo.json\"), \"w\") as f:\n",
    "    json.dump(demo, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wmwXkmK4poD"
   },
   "outputs": [],
   "source": [
    "! python score_d2t.py \\\n",
    "  --dataname chunkingtype \\\n",
    "  --use_demo False \\\n",
    "  --use_ist False \\\n",
    "  --gpt3_score True \\\n",
    "  --gpt3model curie \\\n",
    "  --out_dir_name results_chunkingtype \\\n",
    "  --aspect quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1uo-f0_4AkV"
   },
   "source": [
    "## Plotting Benchmarking Results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

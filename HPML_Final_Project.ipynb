{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HPML Final Project\n",
        "# Authors: Rafka Daou, Maria Garmonina, Sarah Korb\n",
        "# Project 4: Exploring Chunked States in Mamba Style Models\n"
      ],
      "metadata": {
        "id": "oukieAKXF1If"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code blocks handle importing the necessary modules required for the program to run.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ndKDOQT5TIm9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqYnHmrYTfvL"
      },
      "outputs": [],
      "source": [
        "# Install the relevant packages for the project.\n",
        "! pip install -e .\n",
        "\n",
        "! pip install pynvml rouge_score\n",
        "\n",
        "! pip install ibm-fms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import psutil\n",
        "import pynvml\n",
        "import pandas as pd\n",
        "from torch.profiler import profile, ProfilerActivity\n",
        "\n",
        "from fms.utils.tokenizers import get_tokenizer\n",
        "from fms.utils.generation import generate"
      ],
      "metadata": {
        "id": "hrbDNIBnG3kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6Sy_hpjI3kp",
        "outputId": "ca68b9eb-7f93-469a-aa5a-ffa271a97153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounts the notebook to our google drive to access relevant files.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is responsible for loading the IBM Bamba module and configuring key parameters, such as the number of layers the model will use. For the purpose of evaluating throughput and latency, we prioritized a configuration with 4 layers while varying prompt lengths.\n",
        "To address the issue of exploding chunked state strategies, we implemented four distinct module variants: default optimized, optimized, optimized with independent chunking, and optimized diagonal-only.\n",
        "To specify which SSM (Structured State Space Model) module is used during model execution, we manually set the desired module in the statements below."
      ],
      "metadata": {
        "id": "hECsnm-jG-1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import fms.modules.ssm\n",
        "from fms.modules.default_optimized import SSM as DefaultOptimizedSSM\n",
        "\n",
        "fms.modules.ssm.SSM = DefaultOptimizedSSM #overwrite the SSM\n",
        "from fms.models import get_model\n",
        "\n",
        "model = get_model(\n",
        "    \"hf_configured\",\n",
        "    \"ibm-ai-platform/Bamba-9B\",\n",
        "    device_type=\"cuda\",\n",
        "    data_type=torch.bfloat16,\n",
        "    nlayers=4, #specify number of layers\n",
        ")\n",
        "model.config.attn_layer_indices = []\n",
        "\n",
        "\n",
        "print(\"Number of layers:\", len(model.base_model.layers))\n",
        "print(\"Config nlayers:\", model.config.nlayers)\n",
        "print(\"Attention layers indices:\", model.config.attn_layer_indices)\n",
        "\n"
      ],
      "metadata": {
        "id": "VihNjeuo5wgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69W2XZAOpmiN",
        "outputId": "171b6ae0-d1b6-47f2-ff10-86ca9b7d144c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BambaConfig(src_vocab_size=128256, emb_dim=4096, nheads=32, kvheads=8, head_dim=64, norm_eps=1e-05, nlayers=4, activation_fn='silu', attn_layer_indices=[], max_expected_seq_len=262144, ntk_scaling=False, tie_heads=False, rope_theta=10000.0, p_dropout=0.0, conv_kernel=4, state_size=128, hidden_grow_factor=3.5, mamba_expand=2, mamba_n_heads=128, multiple_of=256, use_bias=False, use_conv_bias=True, n_groups=1, chunk_size=256, linear_config=None, fused_weights=True)\n"
          ]
        }
      ],
      "source": [
        "# confirm model parameters\n",
        "print(model.config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm which SSM module is running\n",
        "layer = next(\n",
        "    block for block in model.base_model.layers\n",
        "    if hasattr(block, \"ssm\")\n",
        ")\n",
        "print(isinstance(layer.ssm, ChunkedSSM),\n",
        "      layer.ssm.__class__.__module__ + \".\" + layer.ssm.__class__.__name__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXBm-1y_uM2m",
        "outputId": "e57682a7-e657-455d-9cb1-763beb04e6be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False fms.modules.ssm.SSM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data\n"
      ],
      "metadata": {
        "id": "t1LsKBapJgei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ibm-fms"
      ],
      "metadata": {
        "id": "70v5XQbQJb3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "from fms.utils.tokenizers import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer(\"ibm-ai-platform/Bamba-9B\")\n",
        "\n",
        "with open('/content/drive/My Drive/qa_pairs.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "token_lengths = []\n",
        "\n",
        "for item in data:\n",
        "    prompt = item[0]\n",
        "    tokens = tokenizer.tokenize(prompt)\n",
        "    token_lengths.append(len(tokens))\n",
        "\n",
        "length_counter = Counter(token_lengths)\n",
        "\n",
        "sorted_lengths = dict(sorted(length_counter.items()))\n",
        "\n",
        "print(\"Prompt Length Statistics:\")\n",
        "for length, count in sorted_lengths.items():\n",
        "    if length in [256, 512, 768, 1024, 1280, 1536, 1792, 2048, 2304, 2560, 2816,\n",
        "                  3072, 3328, 3584, 3840, 4096, 4352, 4608, 4864, 5120, 5376,\n",
        "                  5632, 5888, 6144, 6400, 6656, 6912, 7168, 7424, 7680, 7936,\n",
        "                  8192] or (length + 1) in [256, 512, 768, 1024, 1280, 1536, 1792, 2048, 2304, 2560, 2816,\n",
        "                  3072, 3328, 3584, 3840, 4096, 4352, 4608, 4864, 5120, 5376,\n",
        "                  5632, 5888, 6144, 6400, 6656, 6912, 7168, 7424, 7680, 7936,\n",
        "                  8192] or (length + 2) in [256, 512, 768, 1024, 1280, 1536, 1792, 2048, 2304, 2560, 2816,\n",
        "                  3072, 3328, 3584, 3840, 4096, 4352, 4608, 4864, 5120, 5376,\n",
        "                  5632, 5888, 6144, 6400, 6656, 6912, 7168, 7424, 7680, 7936,\n",
        "                  8192] or (length + 3) in [256, 512, 768, 1024, 1280, 1536, 1792, 2048, 2304, 2560, 2816,\n",
        "                  3072, 3328, 3584, 3840, 4096, 4352, 4608, 4864, 5120, 5376,\n",
        "                  5632, 5888, 6144, 6400, 6656, 6912, 7168, 7424, 7680, 7936,\n",
        "                  8192] or (length + 4) in [256, 512, 768, 1024, 1280, 1536, 1792, 2048, 2304, 2560, 2816,\n",
        "                  3072, 3328, 3584, 3840, 4096, 4352, 4608, 4864, 5120, 5376,\n",
        "                  5632, 5888, 6144, 6400, 6656, 6912, 7168, 7424, 7680, 7936,\n",
        "                  8192] or (length + 5) in [256, 512, 768, 1024, 1280, 1536, 1792, 2048, 2304, 2560, 2816,\n",
        "                  3072, 3328, 3584, 3840, 4096, 4352, 4608, 4864, 5120, 5376,\n",
        "                  5632, 5888, 6144, 6400, 6656, 6912, 7168, 7424, 7680, 7936,\n",
        "                  8192]:\n",
        "        print(f\"Length {length}: {count} prompts\")"
      ],
      "metadata": {
        "id": "1eiaJjbEJkuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "from fms.utils.tokenizers import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer(\"ibm-ai-platform/Bamba-9B\")"
      ],
      "metadata": {
        "id": "qa7z4qnIJoqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/qa_pairs.json', 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "2GV8m1flJtrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "length_to_items = defaultdict(list)\n",
        "for item in data:\n",
        "    token_len = len(tokenizer.tokenize(item[0]))\n",
        "    length_to_items[token_len].append(item)"
      ],
      "metadata": {
        "id": "mJ_BcyiBJtgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_config = {\n",
        "    256: {256: 10},\n",
        "    512: {511: 3, 512: 7},\n",
        "    768: {766: 1, 767: 3, 768: 6},\n",
        "    1024: {1023: 4, 1024: 6},\n",
        "    1280: {1275: 2, 1276: 1, 1277: 1, 1278: 2, 1279: 2, 1280: 2},\n",
        "    1536: {1532: 1, 1533: 1, 1534: 2, 1535: 3, 1536: 3},\n",
        "    1792: {1785: 1, 1786: 1, 1788: 3, 1789: 1, 1791: 3, 1792: 1},\n",
        "    2048: {2044: 2, 2045: 5, 2047: 1, 2048: 2},\n",
        "    2304: {2291: 1, 2292: 2, 2294: 2, 2296: 1, 2299: 1, 2301: 1, 2302: 1, 2303: 1},\n",
        "    2560: {2549: 2, 2551: 1, 2553: 1, 2554: 1, 2555: 1, 2556: 2, 2557: 1, 2559: 1},\n",
        "    2816: {2800: 1, 2801: 2, 2802: 1, 2805: 1, 2806: 1, 2810: 2, 2811: 1, 2814: 1},\n",
        "    3072: {3053: 1, 3055: 1, 3057: 1, 3060: 1, 3061: 1, 3063: 1, 3065: 1, 3067: 1, 3070: 1, 3071: 1},\n",
        "    3328: {3302: 1, 3307: 1, 3308: 1, 3310: 2, 3316: 1, 3318: 1, 3323: 2, 3327: 1},\n",
        "    3584: {3534: 1, 3540: 2, 3541: 1, 3543: 1, 3558: 1, 3560: 1, 3563: 1, 3565: 1, 3571: 1}, ##\n",
        "    3840: {3750: 1, 3752: 1, 3773: 1, 3786: 1, 3787: 1, 3789: 1, 3790: 1, 3792: 1, 3813: 1, 3824: 1},\n",
        "    4096: {3969: 1, 3971: 1,3984: 1, 3998: 1, 4014: 1, 4016: 1, 4017: 1, 4040: 1, 4051: 1, 4061: 1},\n",
        "}"
      ],
      "metadata": {
        "id": "Spbj-mikJ8ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_data = []\n",
        "seen_prompts = set()\n",
        "\n",
        "for target_len, sub_lengths in target_config.items():\n",
        "    count = 0\n",
        "    for src_len, take_n in sub_lengths.items():\n",
        "        examples = length_to_items[src_len]\n",
        "        if len(examples) < take_n:\n",
        "            raise ValueError(f\"Not enough prompts of length {src_len} (needed {take_n}, got {len(examples)})\")\n",
        "        used = 0\n",
        "        for item in examples:\n",
        "            if item[0] not in seen_prompts:\n",
        "                seen_prompts.add(item[0])\n",
        "                final_data.append({\n",
        "                    \"prompt\": item[0],\n",
        "                    \"answer\": item[1],\n",
        "                    \"token_len\": src_len,\n",
        "                    \"num_chunks\": math.ceil(src_len / 256)\n",
        "                })\n",
        "                used += 1\n",
        "                count += 1\n",
        "            if used == take_n:\n",
        "                break\n",
        "        if used < take_n:\n",
        "            raise ValueError(f\"Could not find enough unique prompts for length {src_len} (needed {take_n}, got {used})\")\n",
        "    assert count == 10, f\"Expected 10 prompts for length {target_len}, got {count}\""
      ],
      "metadata": {
        "id": "tM0PNTusJ_5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"longer_qa_for_benchmarking.json\", \"w\") as f:\n",
        "    json.dump(final_data, f, indent=2)\n",
        "\n",
        "print(f\"Saved {len(final_data)} prompts to longer_qa_for_benchmarking.json\")"
      ],
      "metadata": {
        "id": "OQFPxPG4KDap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompts for accuracy/quality benchmarking\n",
        "\n",
        "target_config = {\n",
        "    512: {498: 4, 499: 3, 500: 11, 501: 4, 502: 3, 503: 9, 504: 6, 505: 10, 506: 12, 507: 12, 508: 7, 509: 5, 510: 7, 511: 7}\n",
        "}\n"
      ],
      "metadata": {
        "id": "Mso2rjf2KGhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_data = []\n",
        "seen_prompts = set()\n",
        "\n",
        "for target_len, sub_lengths in target_config.items():\n",
        "    count = 0\n",
        "    for src_len, take_n in sub_lengths.items():\n",
        "        examples = length_to_items[src_len]\n",
        "        if len(examples) < take_n:\n",
        "            raise ValueError(f\"Not enough prompts of length {src_len} (needed {take_n}, got {len(examples)})\")\n",
        "        used = 0\n",
        "        for item in examples:\n",
        "            if item[0] not in seen_prompts:\n",
        "                seen_prompts.add(item[0])\n",
        "                final_data.append({\n",
        "                    \"prompt\": item[0],\n",
        "                    \"answer\": item[1],\n",
        "                    \"token_len\": src_len,\n",
        "                    \"num_chunks\": math.ceil(src_len / 256)\n",
        "                })\n",
        "                used += 1\n",
        "                count += 1\n",
        "            if used == take_n:\n",
        "                break\n",
        "        if used < take_n:\n",
        "            raise ValueError(f\"Could not find enough unique prompts for length {src_len} (needed {take_n}, got {used})\")\n",
        "    assert count == 100, f\"Expected 100 prompts for length {target_len}, got {count}\""
      ],
      "metadata": {
        "id": "GlTpxZtkKJmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"qa_for_accuracy512.json\", \"w\") as f:\n",
        "    json.dump(final_data, f, indent=2)\n",
        "\n",
        "print(f\"Saved {len(final_data)} prompts to qa_for_accuracy512.json\")"
      ],
      "metadata": {
        "id": "et2vOmePKNSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_tok = 0\n",
        "for item in final_data:\n",
        "    length_ans = len(tokenizer.tokenize(item['answer']))\n",
        "    max_len_tok = max(max_len_tok, length_ans)\n",
        "\n",
        "print(max_len_tok)"
      ],
      "metadata": {
        "id": "fx4mX5jjKQvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmarking"
      ],
      "metadata": {
        "id": "WmkXI65WGdPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code benchmarks the performance of different chunked state strategies for the Bamba model. It processes a list of prompts by tokenizing the input, running the model to generate output, and collecting detailed performance metrics. These include total latency, first-token and inter-token generation times, throughput (tokens per second), peak memory usage, memory bandwidth, CPU/GPU utilization, and total FLOPs. The profiler also logs the most time-consuming CUDA operations. All results are recorded for analysis, allowing comparison across different SSM module variants and prompt configurations."
      ],
      "metadata": {
        "id": "PUtrjO8vIL7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# updated file containing 16 prompts of lengths [256, 512, 1024, 2048]\n",
        "with open(\"/content/drive/MyDrive/HPML/HPML Project/qa_pairs_for_benchmarking.json\", \"r\",encoding=\"utf-8\") as f:\n",
        "  qa_pairs = json.load(f)"
      ],
      "metadata": {
        "id": "B5lE7gjvSOYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ids_for_prompt(prompt, tokenizer, device):\n",
        "    toks = tokenizer.tokenize(prompt)\n",
        "    ids  = tokenizer.convert_tokens_to_ids(toks)\n",
        "    return torch.tensor(ids, dtype=torch.long, device=device)\n",
        "\n",
        "def decode_ids(ids):\n",
        "    toks  = tokenizer.convert_ids_to_tokens(ids)\n",
        "    return tokenizer.convert_tokens_to_string(toks)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "tokenizer = get_tokenizer(\"ibm-ai-platform/Bamba-9B\")\n",
        "\n",
        "pynvml.nvmlInit()\n",
        "gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "\n",
        "model.compile()\n",
        "\n",
        "records = []\n",
        "MAX_NEW_TOKENS = 100\n",
        "log_path = \"/content/drive/MyDrive/DefaultOptimizedSSM.txt\"\n",
        "\n",
        "with open(log_path, \"a\", encoding=\"utf-8\") as log_file:\n",
        "    for idx, item in enumerate(qa_pairs[:1], start=1): #specify the number of prompts you want to execute on\n",
        "        inputs = ids_for_prompt(item[\"prompt\"], tokenizer, device)\n",
        "\n",
        "        # system stats before\n",
        "        cpu0 = psutil.cpu_percent(None)\n",
        "        io0  = psutil.cpu_times_percent(None).iowait\n",
        "        gpu0 = pynvml.nvmlDeviceGetUtilizationRates(gpu_handle).gpu\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "        # profile the generate step\n",
        "        with profile(\n",
        "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "            record_shapes=True,\n",
        "            profile_memory=True,\n",
        "            with_flops=True,\n",
        "        ) as prof:\n",
        "            torch.cuda.synchronize()\n",
        "            t_start = time.time()\n",
        "\n",
        "            out_ids, times = generate(\n",
        "                model,\n",
        "                inputs,\n",
        "                max_new_tokens=MAX_NEW_TOKENS,\n",
        "                use_cache=False,\n",
        "                timing=\"per-token\",\n",
        "            )\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "            t_end = time.time()\n",
        "\n",
        "        # system stats after\n",
        "        cpu1 = psutil.cpu_percent(None)\n",
        "        io1  = psutil.cpu_times_percent(None).iowait\n",
        "        gpu1 = pynvml.nvmlDeviceGetUtilizationRates(gpu_handle).gpu\n",
        "        peak_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
        "\n",
        "        # derive metrics\n",
        "        t_first     = times[0]\n",
        "        t_mean      = sum(times[1:]) / len(times[1:])\n",
        "        total_time  = t_end - t_start\n",
        "        throughput  = MAX_NEW_TOKENS / total_time\n",
        "        mem_bw      = peak_mem / total_time\n",
        "        total_flops = sum(evt.flops for evt in prof.key_averages() if hasattr(evt, \"flops\"))\n",
        "        top_ops     = prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=5)\n",
        "\n",
        "        rec = {\n",
        "            \"id\": idx,\n",
        "            \"total_latency_s\": total_time,\n",
        "            \"first_token_s\": t_first,\n",
        "            \"mean_inter_token_s\": t_mean,\n",
        "            \"throughput_tok_s\": throughput,\n",
        "            \"peak_mem_MB\": peak_mem,\n",
        "            \"mem_bw_MBps\": mem_bw,\n",
        "            \"cpu_start_%\": cpu0, \"cpu_end_%\": cpu1,\n",
        "            \"gpu_start_%\": gpu0, \"gpu_end_%\": gpu1,\n",
        "            \"io_wait_diff_%\": io1 - io0,\n",
        "            \"total_flops\": total_flops,\n",
        "            \"profiler_top_ops\": top_ops,\n",
        "            \"output\": decode_ids(out_ids),\n",
        "            \"num_chunks\": item[\"num_chunks\"],\n",
        "            \"token_len\": item[\"token_len\"]\n",
        "        }\n",
        "\n",
        "        log_file.write(f\"{rec}\\n\")\n",
        "        records.append(rec)\n",
        "\n",
        "        print(\n",
        "            f\"{idx}/160 | tot={total_time:.3f}s \"\n",
        "            f\"| first={t_first:.3f}s | inter={t_mean:.4f}s | thr={throughput:.1f} tok/s\"\n",
        "        )\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "df.to_csv(\"/content/drive/MyDrive/DefaultOptimizedSSM.csv\", index=False)"
      ],
      "metadata": {
        "id": "qSf4ZT78WFPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuarcy Benchmarking"
      ],
      "metadata": {
        "id": "QqX825G2ID5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code performs benchmarking for evaluating the accuracy of different chunked state strategies in the Bamba model. It processes a list of QA pairs by tokenizing the prompts, generating only the predicted answer portion (based on reference answer length), and decoding the generated output. The results—including prompt, reference answer, and model prediction—are logged to a TSV file for later analysis. This setup isolates the model’s generative accuracy, allowing precise comparisons across chunking strategies while controlling for output length.\n"
      ],
      "metadata": {
        "id": "HLw83_rBId2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "tokenizer = get_tokenizer(\"ibm-ai-platform/Bamba-9B\")\n",
        "model.compile()\n",
        "\n",
        "def ids_for_prompt(prompt: str, tokenizer, device):\n",
        "    toks = tokenizer.tokenize(prompt)\n",
        "    ids = tokenizer.convert_tokens_to_ids(toks)\n",
        "    # prepend BOS if it's different from EOS\n",
        "    if tokenizer.bos_token_id != tokenizer.eos_token_id:\n",
        "        ids = [tokenizer.bos_token_id] + ids\n",
        "    return torch.tensor(ids, dtype=torch.long, device=device)\n",
        "\n",
        "def decode_ids(ids: torch.Tensor):\n",
        "    # ids is a 1D tensor of token ids\n",
        "    toks = tokenizer.convert_ids_to_tokens(ids.tolist())\n",
        "    return tokenizer.convert_tokens_to_string(toks)\n",
        "\n",
        "\n",
        "# CHANGE LOG PATH NAME\n",
        "log_path = \"/content/drive/MyDrive/predictions_chunkingtype.tsv\"\n",
        "write_header = not os.path.exists(log_path)\n",
        "with open(log_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    if write_header:\n",
        "        writer.writerow([\"id\", \"prompt\", \"reference\", \"prediction\"])\n",
        "\n",
        "    for idx, item in enumerate(qa_pairs, start=1):\n",
        "        prompt = item[\"prompt\"]\n",
        "        # tokenize + tensorize\n",
        "        inputs = ids_for_prompt(prompt, tokenizer, device)\n",
        "        prompt_len = inputs.size(0)\n",
        "\n",
        "        # generate just the answer tokens\n",
        "        out_ids = generate(\n",
        "            model,\n",
        "            inputs,\n",
        "            max_new_tokens=len(tokenizer.tokenize(item[\"answer\"])), # only the answer length, (or use 904 on 256 token prompts and 918 on 512 token prompts)\n",
        "            use_cache=False,\n",
        "            timing=\"\",\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        # strip the prompt off the front\n",
        "        new_ids = out_ids[prompt_len:]\n",
        "\n",
        "        # decode only the new tokens\n",
        "        output_text = decode_ids(new_ids)\n",
        "        reference = item[\"answer\"]\n",
        "\n",
        "        writer.writerow([idx, prompt, reference, output_text])\n",
        "\n",
        "        print(f\"{idx:3d}: {output_text[:60]}…\")"
      ],
      "metadata": {
        "id": "tV64hefsY-U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy Benchmaking"
      ],
      "metadata": {
        "id": "m6fH_RlzEAmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section prepares the prediction results for accuracy evaluation using GPTScore, a reference-based automatic metric that leverages GPT models to assess the quality of generated text. GPTScore evaluates how well the system’s predicted answers (summaries) align with human-written references. The code loads the predictions TSV, formats each record into the expected JSON schema, and saves it to the directory. Each entry includes the prompt, reference, prediction, and metadata such as system name and evaluation aspect."
      ],
      "metadata": {
        "id": "McUi0bCbJNKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using gpt scorer [after benchmarking]\n",
        "\n",
        "!git clone https://github.com/jinlanfu/GPTScore.git\n",
        "%cd GPTScore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPQYJrsOhjA8",
        "outputId": "cb4bde74-624f-44f4-cc3d-a152a5f28f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GPTScore'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 50 (delta 13), reused 44 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (50/50), 851.74 KiB | 10.26 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n",
            "/content/foundation-model-stack/GPTScore/GPTScore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# then put the predictions tsv into:\n",
        "\n",
        "import os, json, pandas as pd\n",
        "\n",
        "# file path needs to be updated\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/predictions_chunkingtype.tsv\", sep=\"\\t\")\n",
        "\n",
        "# build the JSON structure\n",
        "records = []\n",
        "for _, row in df.iterrows():\n",
        "    records.append({\n",
        "        \"src\": row[\"prompt\"],\n",
        "        \"ref_summ\": row[\"reference\"],\n",
        "        \"sys_summ\": row[\"prediction\"],\n",
        "        \"sys_name\": \"chunkingtype\",\n",
        "        \"aspect\": \"quality\",\n",
        "        \"polarity\": \"positive\"\n",
        "    })\n",
        "\n",
        "demo = {\n",
        "    \"demo\": {\n",
        "        \"quality\": records\n",
        "    },\n",
        "    \"asp_definition\": {\n",
        "        \"quality\": \"Convert the following text into another expression that is fluent and grammatically correct:\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# write it where the CLI expects it\n",
        "out_dir = \"GPTScore/datas/chunkingtype\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "with open(os.path.join(out_dir, \"demo.json\"), \"w\") as f:\n",
        "    json.dump(demo, f, indent=2)"
      ],
      "metadata": {
        "id": "ZeaWwiym4VnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python score_d2t.py \\\n",
        "  --dataname chunkingtype \\\n",
        "  --use_demo False \\\n",
        "  --use_ist False \\\n",
        "  --gpt3_score True \\\n",
        "  --gpt3model curie \\\n",
        "  --out_dir_name results_chunkingtype \\\n",
        "  --aspect quality"
      ],
      "metadata": {
        "id": "5wmwXkmK4poD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting Benchmarking Results"
      ],
      "metadata": {
        "id": "s1uo-f0_4AkV"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oukieAKXF1If"
      },
      "source": [
        "# Divide, Triton, and Conquer (Independently)\n",
        "## Exploring Chunked States in Bamba\n",
        "### Rafka Daou, Maria Garmonina, Sarah Korb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndKDOQT5TIm9"
      },
      "source": [
        "The following code blocks handle importing the necessary modules required for the program to run.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqYnHmrYTfvL"
      },
      "outputs": [],
      "source": [
        "! pip install -e .\n",
        "\n",
        "! pip install pynvml rouge_score\n",
        "\n",
        "! pip install ibm-fms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wbAX8YgI2fT"
      },
      "source": [
        "Clone Repository\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg1Fki1QI2H9"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/rafkamicheldaou/foundation-model-stack.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jynmkQHcJF3n"
      },
      "outputs": [],
      "source": [
        "# Set working directory and fix sys.path\n",
        "import sys, os\n",
        "\n",
        "os.chdir(\"/content/foundation-model-stack\")\n",
        "sys.path.insert(0, os.getcwd())\n",
        "\n",
        "# Confirm you're using the correct `fms`\n",
        "import fms\n",
        "print(\"Using fms from:\", fms.__file__)  # should point to /content/foundation-model-stack/fms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZWzwQBAM7N1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import psutil\n",
        "import pynvml\n",
        "import pandas as pd\n",
        "from torch.profiler import profile, ProfilerActivity\n",
        "\n",
        "from fms.utils.tokenizers import get_tokenizer\n",
        "from fms.utils.generation import generate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hECsnm-jG-1j"
      },
      "source": [
        "The following code is responsible for loading the IBM Bamba module and configuring key parameters, such as the number of layers the model will use. For the purpose of evaluating throughput and latency, we prioritized a configuration with 4 layers while varying prompt lengths.\n",
        "To address the limitations of state-space models, we implemented two distinct module variants: default optimized, independent.\n",
        "To specify which SSM (State Space Model) module is used during model execution, we manually set the desired module in the statements below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CO7LcjWJT9f"
      },
      "source": [
        "**The model version you use should depend on whether you are benchmarking performance or accuracy. Please ensure you load the appropriate model based on the specific task.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VihNjeuo5wgF"
      },
      "outputs": [],
      "source": [
        "# CODE TO LOAD FOR PERFORMANCE BENCHMARKING\n",
        "from fms.modules.ssm import SSM as DefaultSSM # you can change this to fms.modules.default_optimized_ssm or fms.modules.independent_simple_ssm\n",
        "import fms.models.bamba as _bamba_mod\n",
        "\n",
        "\n",
        "_bamba_mod.SSM = DefaultSSM # assigning this before fetching the pretrained model to not run into errors\n",
        "\n",
        "# Now load Bamba\n",
        "from fms.models import get_model\n",
        "\n",
        "import torch\n",
        "\n",
        "# Load trimmed model properly\n",
        "model = get_model(\n",
        "    \"hf_configured\",\n",
        "    \"ibm-ai-platform/Bamba-9B\",\n",
        "    device_type=\"cuda\",\n",
        "    data_type=torch.bfloat16,\n",
        "    nlayers=4,\n",
        ")\n",
        "model.config.attn_layer_indices = []\n",
        "\n",
        "\n",
        "print(\"Number of layers:\", len(model.base_model.layers))\n",
        "print(\"Config nlayers:\", model.config.nlayers)\n",
        "print(\"Attention layers indices:\", model.config.attn_layer_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeXRlqwXJzrl"
      },
      "outputs": [],
      "source": [
        "# CODE TO LOAD FOR ACCURACY BENCHMARKING\n",
        "from fms.modules.default_triton_ssm import SSM as DefaultSSM # you can change this to fms.modules.default_optimized_ssm or fms.modules.independent_simple_ssm\n",
        "import fms.models.bamba as _bamba_mod\n",
        "\n",
        "\n",
        "_bamba_mod.SSM = DefaultSSM # assigning this before fetching the pretrained model to not run into errors\n",
        "\n",
        "# Now load Bamba\n",
        "from fms.models import get_model\n",
        "\n",
        "import torch\n",
        "model = get_model(\n",
        "    \"hf_pretrained\",\n",
        "    \"ibm-ai-platform/Bamba-9B-v2\",\n",
        "    device_type=\"cuda\",\n",
        ")\n",
        "model.config.attn_layer_indices = []\n",
        "\n",
        "print(\"Number of layers:\", len(model.base_model.layers), flush=True)\n",
        "print(\"Config nlayers:\", model.config.nlayers, flush=True)\n",
        "print(\"Attention layers indices:\", model.config.attn_layer_indices, flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69W2XZAOpmiN"
      },
      "outputs": [],
      "source": [
        "# confirm model parameters\n",
        "print(model.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXBm-1y_uM2m"
      },
      "outputs": [],
      "source": [
        "# confirm which SSM module is running\n",
        "layer = next(\n",
        "    block for block in model.base_model.layers\n",
        "    if hasattr(block, \"ssm\")\n",
        ")\n",
        "print(isinstance(layer.ssm, DefaultSSM), # PASS IN THE LOADED SSM MODULE HERE TO VERIFY IT IS BEING LOADED PROPERLY\n",
        "      layer.ssm.__class__.__module__ + \".\" + layer.ssm.__class__.__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmkXI65WGdPA"
      },
      "source": [
        "## Performance Benchmarking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUtrjO8vIL7t"
      },
      "source": [
        "This code benchmarks the performance of different chunked state strategies for the Bamba model. It processes a list of prompts by tokenizing the input, running the model to generate output, and collecting detailed performance metrics. These include total latency, first-token and inter-token generation times, throughput (tokens per second), peak memory usage, memory bandwidth, CPU/GPU utilization, and total FLOPs. The profiler also logs the most time-consuming CUDA operations. All results are recorded for analysis, allowing comparison across different SSM module variants and prompt configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5lE7gjvSOYq"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# prompts of varying lengths\n",
        "with open(\"./benchmarking_data/longer_qa_for_benchmarking_performance.json\", \"r\",encoding=\"utf-8\") as f:\n",
        "  qa_pairs = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSf4ZT78WFPY"
      },
      "outputs": [],
      "source": [
        "def ids_for_prompt(prompt, tokenizer, device):\n",
        "    toks = tokenizer.tokenize(prompt)\n",
        "    ids  = tokenizer.convert_tokens_to_ids(toks)\n",
        "    return torch.tensor(ids, dtype=torch.long, device=device)\n",
        "\n",
        "def decode_ids(ids):\n",
        "    toks  = tokenizer.convert_ids_to_tokens(ids)\n",
        "    return tokenizer.convert_tokens_to_string(toks)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "tokenizer = get_tokenizer(\"ibm-ai-platform/Bamba-9B\")\n",
        "\n",
        "pynvml.nvmlInit()\n",
        "gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "\n",
        "model.compile()\n",
        "\n",
        "records = []\n",
        "MAX_NEW_TOKENS = 100\n",
        "log_path = \"./benchmarking_data/performance_default.txt\" # change the log path name\n",
        "with open(log_path, \"a\", encoding=\"utf-8\") as log_file:\n",
        "    for idx, item in enumerate(qa_pairs[:70], start=1):\n",
        "        inputs = ids_for_prompt(item[\"prompt\"], tokenizer, device)\n",
        "\n",
        "        # system stats before\n",
        "        cpu0 = psutil.cpu_percent(None)\n",
        "        io0  = psutil.cpu_times_percent(None).iowait\n",
        "        gpu0 = pynvml.nvmlDeviceGetUtilizationRates(gpu_handle).gpu\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "        # profile the generate step\n",
        "        with profile(\n",
        "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "            record_shapes=True,\n",
        "            profile_memory=True,\n",
        "            with_flops=True,\n",
        "        ) as prof:\n",
        "            torch.cuda.synchronize()\n",
        "            t_start = time.time()\n",
        "\n",
        "            out_ids, times = generate(\n",
        "                model,\n",
        "                inputs,\n",
        "                max_new_tokens=MAX_NEW_TOKENS,\n",
        "                use_cache=False,\n",
        "                timing=\"per-token\",\n",
        "            )\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "            t_end = time.time()\n",
        "\n",
        "        # system stats after\n",
        "        cpu1 = psutil.cpu_percent(None)\n",
        "        io1  = psutil.cpu_times_percent(None).iowait\n",
        "        gpu1 = pynvml.nvmlDeviceGetUtilizationRates(gpu_handle).gpu\n",
        "        peak_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
        "\n",
        "        # derive metrics\n",
        "        t_first     = times[0]\n",
        "        t_mean      = sum(times[1:]) / len(times[1:])\n",
        "        total_time  = t_end - t_start\n",
        "        throughput  = MAX_NEW_TOKENS / total_time\n",
        "        mem_bw      = peak_mem / total_time\n",
        "        total_flops = sum(evt.flops for evt in prof.key_averages() if hasattr(evt, \"flops\"))\n",
        "        top_ops     = prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=5)\n",
        "\n",
        "        rec = {\n",
        "            \"id\": idx,\n",
        "            \"total_latency_s\": total_time,\n",
        "            \"first_token_s\": t_first,\n",
        "            \"mean_inter_token_s\": t_mean,\n",
        "            \"throughput_tok_s\": throughput,\n",
        "            \"peak_mem_MB\": peak_mem,\n",
        "            \"mem_bw_MBps\": mem_bw,\n",
        "            \"cpu_start_%\": cpu0, \"cpu_end_%\": cpu1,\n",
        "            \"gpu_start_%\": gpu0, \"gpu_end_%\": gpu1,\n",
        "            \"io_wait_diff_%\": io1 - io0,\n",
        "            \"total_flops\": total_flops,\n",
        "            \"profiler_top_ops\": top_ops,\n",
        "            \"output\": decode_ids(out_ids),\n",
        "            \"num_chunks\": item[\"num_chunks\"],\n",
        "            \"token_len\": item[\"token_len\"],\n",
        "            \"prompt\": item[\"prompt\"]\n",
        "        }\n",
        "\n",
        "        log_file.write(f\"{rec}\\n\")\n",
        "        records.append(rec)\n",
        "\n",
        "        print(\n",
        "            f\"{idx}/160 | tot={total_time:.3f}s \"\n",
        "            f\"| first={t_first:.3f}s | inter={t_mean:.4f}s | thr={throughput:.1f} tok/s\",flush=True\n",
        "        )\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "df.to_csv(\"./benchamrking_data/performance_default.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqX825G2ID5j"
      },
      "source": [
        "# Accuarcy Benchmarking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLw83_rBId2C"
      },
      "source": [
        "This code performs benchmarking for evaluating the accuracy of different chunked state strategies in the Bamba model. It processes a list of QA pairs by tokenizing the prompts, generating only the predicted answer portion (based on reference answer length), and decoding the generated output. The results—including prompt, reference answer, and model prediction—are logged to a TSV file for later analysis. This setup isolates the model’s generative accuracy, allowing precise comparisons across chunking strategies while controlling for output length.\n",
        "\n",
        "Please refer to the notebook `GPTScore.ipynb` for accuracy evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-L2vEmgUWVsx"
      },
      "outputs": [],
      "source": [
        "with open(\"./benchmarking_data/qa_for_accuracy_256.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    qa_pairs = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tV64hefsY-U3"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "tokenizer = get_tokenizer(\"ibm-ai-platform/Bamba-9B\")\n",
        "model.compile()\n",
        "\n",
        "def ids_for_prompt(prompt: str, tokenizer, device):\n",
        "    toks = tokenizer.tokenize(prompt)\n",
        "    ids = tokenizer.convert_tokens_to_ids(toks)\n",
        "    if tokenizer.bos_token_id != tokenizer.eos_token_id:\n",
        "        ids = [tokenizer.bos_token_id] + ids\n",
        "    return torch.tensor(ids, dtype=torch.long, device=device)\n",
        "\n",
        "def decode_ids(ids: torch.Tensor):\n",
        "    toks = tokenizer.convert_ids_to_tokens(ids.tolist())\n",
        "    return tokenizer.convert_tokens_to_string(toks)\n",
        "\n",
        "log_path = \"./benchmarking_data/accuracy_default.tsv\" # change the name of the log path\n",
        "write_header = not os.path.exists(log_path)\n",
        "\n",
        "with open(log_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    if write_header:\n",
        "        writer.writerow([\"id\", \"prompt\", \"reference\", \"prediction\"])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, item in enumerate(qa_pairs[63:73], start=1):\n",
        "            prompt = item[\"prompt\"]\n",
        "            inputs = ids_for_prompt(prompt, tokenizer, device)\n",
        "            prompt_len = inputs.size(0)\n",
        "            prompt_len = inputs.size(0)\n",
        "            answer_len = len(tokenizer.tokenize(item[\"answer\"]))\n",
        "            total_len = prompt_len + answer_len\n",
        "\n",
        "            print(f\"Prompt {idx} | Prompt tokens: {prompt_len} | Answer tokens: {answer_len} | Total: {total_len}\",flush=True)\n",
        "            out_ids = generate(\n",
        "                model,\n",
        "                inputs,\n",
        "                max_new_tokens=len(tokenizer.tokenize(item[\"answer\"])),\n",
        "                use_cache=True,\n",
        "                timing=\"\",\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "            new_ids = out_ids[prompt_len:]\n",
        "            output_text = decode_ids(new_ids)\n",
        "            reference = item[\"answer\"]\n",
        "\n",
        "            writer.writerow([idx, prompt, reference, output_text])\n",
        "            print(f\"{idx:3d}: {output_text}…\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
